{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nlp-preprocessing text-preprocessing provides text preprocessing functions i.e. text cleaning, dataset preprocessing, tokenization etc Installation pip install nlp_preprocessing","title":"Home"},{"location":"#nlp-preprocessing","text":"text-preprocessing provides text preprocessing functions i.e. text cleaning, dataset preprocessing, tokenization etc","title":"nlp-preprocessing"},{"location":"#installation","text":"pip install nlp_preprocessing","title":"Installation"},{"location":"cleaning/","text":"Text Cleaning The nlp_preprocessing.clean module allow efficient way of text cleaning. from nlp_preprocessing.clean import * Clean class initiator takes ordered list of cleaning functions(list given below) and using __call__ function we can apply each function on list of string. CLEAN_FUNS = { 'to_lower' : to_lower(), 'to_normalize' : to_normalize(), 'remove_href' : remove_href(), 'remove_control_char' : remove_control_char(), 'remove_duplicate' : remove_duplicate(), 'remove_underscore' : remove_underscore(), 'seperate_spam_chars' : seperate_spam_chars(), 'seperate_brakets_quotes' : seperate_brakets_quotes(), 'break_short_words' : break_short_words(), 'break_long_words' : break_long_words(), 'remove_ending_underscore' : remove_ending_underscore(), 'remove_starting_underscore' : remove_starting_underscore(), 'seperate_end_word_punctuations' : seperate_end_word_punctuations(), 'seperate_start_word_punctuations' : seperate_start_word_punctuations(), 'clean_contractions' : clean_contractions(), 'remove_s' : remove_s(), 'isolate_numbers' : isolate_numbers(), 'regex_split_word' : regex_split_word(), 'leet_clean' : leet_clean(), 'clean_open_holded_words' : clean_open_holded_words(), 'clean_multiple_form' : clean_multiple_form() } class Clean [source] Clean ( clean_fn_ordered_list = ['to_lower', 'to_normalize', 'remove_href', 'remove_control_char', 'remove_duplicate', 'remove_underscore', 'seperate_spam_chars', 'seperate_brakets_quotes', 'break_short_words', 'break_long_words', 'remove_ending_underscore', 'remove_starting_underscore', 'seperate_end_word_punctuations', 'seperate_start_word_punctuations', 'clean_contractions', 'remove_s', 'isolate_numbers', 'regex_split_word', 'leet_clean', 'clean_open_holded_words', 'clean_multiple_form'] ) Examples: texts = ['Hi, how are you', \"I am's good\"] cleaned_text = Clean()(texts) print('Output :', cleaned_text) ########## Step - Lowering everything: ########## Step - Normalize chars and dots: ########## Step - Remove hrefs: ########## Step - Control Chars: ########## Step - Duplicated Chars: Total Words : 7 ########## Step - Remove underscore: Total Words : 7 ['hi, how are you'] ########## Step - Spam chars repetition: Total Words : 7 {} ########## Step - Brackets and quotes: ########## Step - Break long words: ########## Step - Break long words: ########## Step - Remove ending underscore: Total Words : 7 ########## Step - Remove starting underscore: Total Words : 7 ########## Step - End word punctuations: hi, --- hi , ########## Step - Start word punctuations: ########## Step - Contractions: Total Words : 8 ########## Step - Remove \"s: Total Words : 8 am's --- am ########## Step - Isolate numbers: Total Words : 8 Total Words : 8 , --- , ########## Step - L33T (with vocab check): Total Words : 8 ########## Step - Open Holded words: ########## Step - Multiple form: Total Words : 8 Output : ['hi , how are you', 'i am good'] texts = ['Hi, how are you', \"I am's good\"] cleaned_text = Clean(['to_lower','remove_s','seperate_end_word_punctuations'])(texts) print('Output :', cleaned_text) ########## Step - Lowering everything: ########## Step - Remove \"s: Total Words : 7 am's --- am ########## Step - End word punctuations: hi, --- hi , Output : ['hi , how are you', 'i am good'] All cleaning functions takes list of string as input and map function on it to_lower [source] to_lower ( data ) to_normalize [source] to_normalize ( data ) remove_href [source] remove_href ( data ) remove_control_char [source] remove_control_char ( data ) remove_duplicate [source] remove_duplicate ( data ) remove_underscore [source] remove_underscore ( data ) seperate_spam_chars [source] seperate_spam_chars ( data ) seperate_brakets_quotes [source] seperate_brakets_quotes ( data ) break_short_words [source] break_short_words ( data ) break_long_words [source] break_long_words ( data ) remove_ending_underscore [source] remove_ending_underscore ( data ) remove_starting_underscore [source] remove_starting_underscore ( data ) seperate_end_word_punctuations [source] seperate_end_word_punctuations ( data ) seperate_start_word_punctuations [source] seperate_start_word_punctuations ( data ) clean_contractions [source] clean_contractions ( data ) remove_s [source] remove_s ( data ) isolate_numbers [source] isolate_numbers ( data ) regex_split_word [source] regex_split_word ( data ) leet_clean [source] leet_clean ( data ) clean_open_holded_words [source] clean_open_holded_words ( data ) clean_multiple_form [source] clean_multiple_form ( data )","title":"Cleaning"},{"location":"cleaning/#text-cleaning","text":"The nlp_preprocessing.clean module allow efficient way of text cleaning. from nlp_preprocessing.clean import * Clean class initiator takes ordered list of cleaning functions(list given below) and using __call__ function we can apply each function on list of string. CLEAN_FUNS = { 'to_lower' : to_lower(), 'to_normalize' : to_normalize(), 'remove_href' : remove_href(), 'remove_control_char' : remove_control_char(), 'remove_duplicate' : remove_duplicate(), 'remove_underscore' : remove_underscore(), 'seperate_spam_chars' : seperate_spam_chars(), 'seperate_brakets_quotes' : seperate_brakets_quotes(), 'break_short_words' : break_short_words(), 'break_long_words' : break_long_words(), 'remove_ending_underscore' : remove_ending_underscore(), 'remove_starting_underscore' : remove_starting_underscore(), 'seperate_end_word_punctuations' : seperate_end_word_punctuations(), 'seperate_start_word_punctuations' : seperate_start_word_punctuations(), 'clean_contractions' : clean_contractions(), 'remove_s' : remove_s(), 'isolate_numbers' : isolate_numbers(), 'regex_split_word' : regex_split_word(), 'leet_clean' : leet_clean(), 'clean_open_holded_words' : clean_open_holded_words(), 'clean_multiple_form' : clean_multiple_form() }","title":"Text Cleaning "},{"location":"dataset/","text":"from nlp_preprocessing.dataset import Dataset Dataset allow to split in test-train and encode (label or one-hot). It allow both multi-label and multi-class split class Dataset [source] Dataset ( data_config ) Dataset allow split and encoding using external config file Args: data_config (dict): config dict example: data_config = { 'data_class':'multi-class', 'x_columns':[], 'y_columns':[], 'one_hot_encoded_columns':[], 'label_encoded_columns':[], 'data':None, 'split_ratio':0.2, 'random_state':3107 } where data_class: ['multi-class','multi-label'] and data: it should be dataframe Example import pandas as pd text = ['I am Test 1','I am Test 2'] label = ['A','B'] aspect = ['C','D'] data = pd.DataFrame({'text':text*5,'label':label*5,'aspect':aspect*5}) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } text label aspect 0 I am Test 1 A C 1 I am Test 2 B D 2 I am Test 1 A C 3 I am Test 2 B D 4 I am Test 1 A C 5 I am Test 2 B D 6 I am Test 1 A C 7 I am Test 2 B D 8 I am Test 1 A C 9 I am Test 2 B D data_config = { 'data_class':'multi-class', 'x_columns':['text','aspect'], 'y_columns':['label'], 'one_hot_encoded_columns':[], 'label_encoded_columns':['label','aspect'], 'data':data, 'split_ratio':0.2 } dataset = Dataset(data_config) dataset.data_config {'data_class': 'multi-class', 'x_columns': ['text', 'aspect'], 'y_columns': ['label'], 'one_hot_encoded_columns': [], 'label_encoded_columns': ['label', 'aspect'], 'data': text label aspect 0 I am Test 1 A C 1 I am Test 2 B D 2 I am Test 1 A C 3 I am Test 2 B D 4 I am Test 1 A C 5 I am Test 2 B D 6 I am Test 1 A C 7 I am Test 2 B D 8 I am Test 1 A C 9 I am Test 2 B D, 'split_ratio': 0.2, 'random_state': 3107} train, test = dataset.get_train_test_data() train['Y_train'],train['X_train'] ({'label': array([0, 1, 1, 0, 0, 1, 0, 1])}, {'text': array(['I am Test 1', 'I am Test 2', 'I am Test 2', 'I am Test 1', 'I am Test 1', 'I am Test 2', 'I am Test 1', 'I am Test 2'], dtype=object), 'aspect': array([0, 1, 1, 0, 0, 1, 0, 1])}) test['Y_test'],test['X_test'] ({'label': array([0, 1])}, {'text': array(['I am Test 1', 'I am Test 2'], dtype=object), 'aspect': array([0, 1])})","title":"Dataset"},{"location":"tokenization/","text":"from nlp_preprocessing.seq_token_generator import * SpacyTokenizer allow to tokenize your text. __call__ method can class SpacyTokenizer [source] SpacyTokenizer ( vocab_file = None , spacy_tokenizer = <spacy.tokenizer.Tokenizer object at 0x167c88950> , special_token = ['[PAD]'] , pad_token_index = 0 ) SpacyTokenizer.__call__ [source] SpacyTokenizer. call ( inputs , call_type = 'tokenize' , max_seq = None ) __call__ method allow to call encode, encode_plus and tokenize from single interface. Args: inputs (List or string): Input can be string or list of text call_type (str, optional): can be encode, encode_plus, tokenize. Defaults to 'tokenize'. max_seq ([type], optional): it applies for encode and encode_plus call_type Defaults to None (for tokenzie call_type). Returns: tokens or ids: List or List of List SpacyTokenizer.encode [source] SpacyTokenizer.encode ( text , max_seq = 128 ) encode method allow to encode text into ids with max_seq lenght Args: text (string): input text max_seq (int, optional): Defaults to 128. Returns: tokens: List of token SpacyTokenizer.encode_plus [source] SpacyTokenizer.encode_plus ( input_texts , max_seq = 128 ) encode_plus method allow to encode list of text into list of ids with max_seq lenght Args: input_texts (List): List of text max_seq (int, optional): Defaults to 128. Returns: tokens: List of List of token SpacyTokenizer.tokenize [source] SpacyTokenizer.tokenize ( input_texts ) tokenizer method allow to tokenize text Args: input_texts (List): Takes list of text(string) Returns: tokens: List[List] Examples texts = ['Hi, how are you', \"I am good\"] tokens = SpacyTokenizer()(texts, call_type='tokenize') print('Output :',tokens) 2it [00:00, 4.64it/s] Output : [['Hi', ',', 'how', 'are', 'you'], ['I', 'am', 'good']] texts = ['Hi, how are you', \"I am good\"] spacy_tokenizer = SpacyTokenizer() tokens = spacy_tokenizer.tokenize(texts) ids = [spacy_tokenizer.convert_tokens_to_ids(token) for token in tokens] print('Tokens : ',tokens) print('Token_ids : ', ids) print('Vocab : ', spacy_tokenizer.vocab) 2it [00:00, 79.79it/s] Tokens : [['Hi', ',', 'how', 'are', 'you'], ['I', 'am', 'good']] Token_ids : [[1, 2, 3, 4, 5], [6, 7, 8]] Vocab : {'[PAD]': 0, 'Hi': 1, ',': 2, 'how': 3, 'are': 4, 'you': 5, 'I': 6, 'am': 7, 'good': 8} texts = ['Hi, how are you', \"I am good\"] spacy_tokenizer = SpacyTokenizer() ids = spacy_tokenizer.encode_plus(texts, max_seq=10) print('Token_ids : ', ids) print('Vocab : ', spacy_tokenizer.vocab) 2it [00:00, 80.31it/s] Token_ids : [[1, 2, 3, 4, 5, 0, 0, 0, 0, 0], [6, 7, 8, 0, 0, 0, 0, 0, 0, 0]] Vocab : {'[PAD]': 0, 'Hi': 1, ',': 2, 'how': 3, 'are': 4, 'you': 5, 'I': 6, 'am': 7, 'good': 8} spacy_tokenizer = SpacyTokenizer() ids = spacy_tokenizer.encode('Hi, how are you', max_seq=10) print('Token Ids :', ids) print('Vocab : ', spacy_tokenizer.vocab) Token Ids : [1, 2, 3, 4, 5, 0, 0, 0, 0, 0] Vocab : {'[PAD]': 0, 'Hi': 1, ',': 2, 'how': 3, 'are': 4, 'you': 5} from nlp_preprocessing.seq_parser_token_generator import * SpacyParseTokenizer allow to tokenize text and get different parse tokens i.e. dependency parse, tag parse, pos parse from Spacy model class SpacyParseTokenizer [source] SpacyParseTokenizer ( parsers = ['pos', 'tag', 'dep'] ) SpacyParseTokenizer.__call__ [source] SpacyParseTokenizer. call ( inputs , call_type = 'tokenize' , max_seq = None ) __call__ method allow a single interface to call encode, encode_plus and tokenize methods Args: inputs (List or string): It can be string (for encode call type) or List for encode_plus and tokenize call_type (str, optional): can be encode, encode_plus, tokenize. Defaults to 'tokenize'. max_seq ([type], optional): it applies for encode and encode_plus call_type Defaults to None (for tokenzie call_type). Returns: results: dict (contains keys i.e. tag, pos, dep) SpacyParseTokenizer.tokenize [source] SpacyParseTokenizer.tokenize ( input_texts ) tokenizer method allow to tokenize text Args: input_texts (List): Takes list of text(string) Returns: results: dict SpacyParseTokenizer.encode [source] SpacyParseTokenizer.encode ( text , max_seq = 128 ) encode method allow to encode text into ids with max_seq lenght Args: text (string): input text max_seq (int, optional): Defaults to 128. Returns: results: dict SpacyParseTokenizer.encode_plus [source] SpacyParseTokenizer.encode_plus ( input_texts , max_seq = 128 ) encode_plus method allow to encode list of text into list of ids with max_seq lenght Args: input_texts (List): List of text max_seq (int, optional): Defaults to 128. Returns: results: dict Examples texts = ['Hi, how are you', \"I am good\"] tokens = SpacyParseTokenizer()(texts, call_type='tokenize') print('Output :',tokens) 2it [00:00, 51.19it/s] Output : {'pos': [['INTJ', 'PUNCT', 'ADV', 'AUX', 'PRON'], ['PRON', 'AUX', 'ADJ']], 'tag': [['UH', ',', 'WRB', 'VBP', 'PRP'], ['PRP', 'VBP', 'JJ']], 'dep': [['intj', 'punct', 'advmod', 'ROOT', 'nsubj'], ['nsubj', 'ROOT', 'acomp']]} texts = ['Hi, how are you', \"I am good\"] tokens = SpacyTokenizer()(texts, call_type='tokenize') parse_tokens = SpacyParseTokenizer()(texts, call_type='tokenize') print('Output : ',tokens) print('Parse Dict : ', parse_tokens) 2it [00:00, 70.61it/s] 2it [00:00, 54.82it/s] Output : [['Hi', ',', 'how', 'are', 'you'], ['I', 'am', 'good']] Parse Dict : {'pos': [['INTJ', 'PUNCT', 'ADV', 'AUX', 'PRON'], ['PRON', 'AUX', 'ADJ']], 'tag': [['UH', ',', 'WRB', 'VBP', 'PRP'], ['PRP', 'VBP', 'JJ']], 'dep': [['intj', 'punct', 'advmod', 'ROOT', 'nsubj'], ['nsubj', 'ROOT', 'acomp']]}","title":"Tokenization"},{"location":"vocab_embedding_extractor/","text":"from nlp_preprocessing.vocab_embedding_extractor import VocabEmbeddingExtractor VocabEmbeddingExtractor allow to extract vocab and corrosponding embeddings from word2vec, fasttext, glove etc. word embeddings class VocabEmbeddingExtractor [source] VocabEmbeddingExtractor ( vector_file , input_file , column_name ) VocabEmbeddingExtractor takes files(input file, vector file) and extract vocab and corrosponding embeddings from word2vec, fasttext, glove etc. word embeddings Args: vector_file (string): external vector file i.e. word2vec, glove, fasttext input_file (string): input file in csv column_name (string): text column name from input file VocabEmbeddingExtractor.process [source] VocabEmbeddingExtractor.process ( output_dir , special_tokens = [] ) process method allow to process and save output to output_dir Args: output_dir (string): output directory special_tokens (list of string, optional): List all special tokens i.e [PAD], [SEP] . Defaults to []. Example vector_file='../input/fasttext-crawl-300d-2m-with-subword/crawl-300d-2m-subword/crawl-300d-2M-subword.vec' input_file='../input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv' column_name='text' extractor = VocabEmbeddingExtractor(vector_file, input_file, column_name) output_dir = '.' special_tokens = ['[UNK]','[SEP]'] extractor.process(output_dir, special_tokens)","title":"Vocab & Embedding Extractor"}]}